import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

import logging
logging.basicConfig(level=logging.INFO)

import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig,
)
from transformers.trainer_utils import get_last_checkpoint
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import torch.nn.functional as F

# âœ… ëª¨ë¸ê³¼ ì¶œë ¥ ê²½ë¡œ
model_name = "MLP-KTLim/llama-3-Korean-Bllossom-8B"
output_dir = "./experiments/test-emoti-real/emoti-lora-8b-v3"  # (ë²„ì „ êµ¬ë¶„)
dataset_name = "sseyeonn/emoti-chatbot-ko"
data_files = [f"d_cactus{i}_ko_final.jsonl" for i in range(1, 17)]  # 1~16

# âœ… ë°ì´í„°ì…‹ ë¡œë“œ
dataset = load_dataset(dataset_name, data_files={"train": data_files}, split="train")

# âœ… í”„ë¡¬í”„íŠ¸ ìƒì„±
def format_instruction(example):
    return {"text": example["dialogue"]}

dataset = dataset.map(format_instruction)

# âœ… í† í¬ë‚˜ì´ì €
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)

# âœ… pad_token ì—†ìœ¼ë©´ ì¶”ê°€
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
tokenizer.padding_side = "left"

def tokenize(example):
    output = tokenizer(
        example["text"],
        padding="max_length",
        truncation=True,
        max_length=4096,
    )
    output["labels"] = output["input_ids"].copy()
    return output

tokenized_dataset = dataset.map(tokenize, remove_columns=dataset.column_names)

# âœ… QLoRA ì„¤ì •
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)

# âœ… í† í° ì¶”ê°€ ì‹œ embedding resize í•„ìš”
model.resize_token_embeddings(len(tokenizer))

model = prepare_model_for_kbit_training(model)
model.gradient_checkpointing_enable()

# âœ… LoRA ì„¤ì • (ğŸ” ë³€ê²½ëœ ë¶€ë¶„: target_modules â†’ ["q_proj", "v_proj"])
peft_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],  # âœ… ì—¬ê¸° ë³€ê²½
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)
model = get_peft_model(model, peft_config)

# âœ… TrainingArguments (ğŸ” learning_rate â†’ 1e-4 ë¡œ ë³€ê²½)
training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=32,
    num_train_epochs=5,
    learning_rate=1e-4,  # âœ… ì—¬ê¸° ë³€ê²½
    lr_scheduler_type="cosine",
    weight_decay=0.01,
    logging_strategy="steps",
    logging_steps=50,
    bf16=True,
    fp16=False,
    gradient_checkpointing=True,
    save_strategy="steps",
    save_steps=100,
    save_total_limit=3,
    load_best_model_at_end=False,
    report_to="none",
)

# âœ… CustomTrainer
class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        outputs = model(**inputs)
        logits = outputs.get("logits")
        labels = inputs.get("labels")
        loss = F.cross_entropy(
            logits.view(-1, logits.size(-1)),
            labels.view(-1),
            ignore_index=tokenizer.pad_token_id,
        )
        return (loss, outputs) if return_outputs else loss

trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
)

# âœ… ì²´í¬í¬ì¸íŠ¸ ì´ì–´ë°›ê¸°
last_checkpoint = get_last_checkpoint(output_dir)
if last_checkpoint is not None:
    print(f"ğŸ“¦ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ì‹œì‘: {last_checkpoint}")
    trainer.train(resume_from_checkpoint=last_checkpoint)
else:
    print("ğŸ“¦ ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤")
    trainer.train()

# âœ… ì–´ëŒ‘í„°ì™€ í† í¬ë‚˜ì´ì € ì €ì¥
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

print("\nğŸ‰ íŒŒì¸íŠœë‹ ì™„ë£Œ! LoRA adapterê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:", output_dir)
